{"config": {"git_hash": "cb649363302173747a4a0ab9b61635981dc7c3ad", "git_diff": "diff --git a/fold_opt/fold_opt.py b/fold_opt/fold_opt.py\nindex 734edfc..bf3d6b3 100644\n--- a/fold_opt/fold_opt.py\n+++ b/fold_opt/fold_opt.py\n@@ -130,7 +130,7 @@ def JgP_LFPI(c, x_star_step, x_star_blank, g, n_steps = 1000, solver=None):\n \n     N = x_star_blank.shape[1]\n     B = x_star_blank.shape[0]\n-\n+        \n     v = torch.autograd.grad(x_star_step, x_star_blank, g, retain_graph=True)[0].detach()\n \n     for i in range(n_steps):\ndiff --git a/multilabel_example/.ipynb_checkpoints/multilabel_models-checkpoint.py b/multilabel_example/.ipynb_checkpoints/multilabel_models-checkpoint.py\nindex b171457..50323ef 100644\n--- a/multilabel_example/.ipynb_checkpoints/multilabel_models-checkpoint.py\n+++ b/multilabel_example/.ipynb_checkpoints/multilabel_models-checkpoint.py\n@@ -10,6 +10,7 @@ from cvxpylayers.torch.cvxpylayer import CvxpyLayer\n import cvxpy as cp\n \n from multilabel_example.qpth_plus.qpth.qp import QPFunction\n+from multilabel_example.qpth_plus.qpth.qp import QPFunction_Plus\n \n \n \n@@ -17,10 +18,7 @@ from multilabel_example.qpth_plus.qpth.qp import QPFunction\n def obj_grad_fn(x,c,eps):\n     return -c + eps*(torch.log(x) + 1.0 )\n \n-def fwd_solver(c, N, C, eps):\n-    # c = c.clone().cpu().numpy()\n-    # print(type(c), c.shape)\n-    \n+def fwd_solver(c, N, C, eps, primaldual=False):\n     x = cp.Variable(N)\n     constraints = [  0<=x, x<=1, cp.sum(x) == C  ]\n     problem = cp.Problem(cp.Maximize(c @ x + eps*cp.sum(cp.entr(x))), constraints)\n@@ -28,7 +26,9 @@ def fwd_solver(c, N, C, eps):\n     solution = problem.solve(solver=cp.ECOS)\n     primal   = x.value\n     dual = np.concatenate( tuple([np.array([constr.dual_value]).flatten() for constr in constraints]) )\n-    return torch.Tensor(primal), torch.Tensor(dual)\n+    \n+    if primaldual: return torch.Tensor(primal), torch.Tensor(dual)\n+    return torch.Tensor(primal)\n \n # def get_projection_layer(A,b,G,h):\n #     N = A.shape[1] # number of variables\n@@ -41,18 +41,56 @@ def fwd_solver(c, N, C, eps):\n #     cvxlayer_post = lambda z: cvxlayer(z)[0]\n #     return cvxlayer_post\n \n+\n def get_projection_layer(A,b,G,h):\n     N = A.shape[1] # number of variables  \n     Q = torch.eye(N)\n     return lambda x: QPFunction(verbose=-1,eps=1e-10,maxIter=1000)(2*Q.double(),-2*x.double(),G.double(),h.double(),A.double(),b.double())   \n \n+def get_sqp_layer(g, e, grad_g, grad_e, grad_f, hess_L):\n+    \n+    def sqp_layer(x,c,lam):\n+        G = torch.stack( [grad_g(x_i).double() for x_i in x] )\n+        h = torch.stack( [-g(x_i).double() for x_i in x] )\n+        if e != None:\n+            A = torch.stack( [grad_e(x_i).double() for x_i in x] )\n+            b = torch.stack( [-e(x_i).double() for x_i in x] )\n+        else:\n+            A = torch.Tensor([])\n+            b = torch.Tensor([])\n+\n+        Q = torch.stack( [hess_L(x[i],lam[i],c[i] ).double()    for i in range(len(x))] )\n+        p = torch.stack( [grad_f(x_i,c[i]).double() for i,x_i in enumerate(x)] )\n+\n+        #The Hessian is H=2Q, the SQP minimizes (1/2)xT H x, Amos puts in (1/2) automatically\n+        primaldual = QPFunction_Plus(verbose=-1,eps=1e-14,maxIter=1000)(Q,p,G,h,A,b)\n+        return primaldual\n+    \n+    return sqp_layer\n+\n+\n # All data inputs are expected to be torch Tensors\n-def PGD(x0,c,eps,projection,obj_grad,alpha=0.01,n_iter=1):\n+def PGD(x0,c,projection,obj_grad,alpha=0.01,n_iter=1):\n     xi = x0\n     for i in range(n_iter):\n         grads = obj_grad(xi,c)\n         xi = projection(xi - alpha*grads)\n     return xi\n+\n+\n+def SQP(x0,fwd,c,projection,N,M_in,M_eq,alpha=0.01,n_iter=1):\n+    primaldual = fwd(c.clone().detach(), dual=True)\n+    xi = x0\n+    lam = primaldual[:,N:N+M_in]\n+    for i in range(n_iter):\n+        primaldual = projection(xi,c,lam)\n+        d          = primaldual[ :,:N ]\n+        lam_d      = primaldual[ :, N:N+M_in ]\n+        nu_d       = primaldual[ :,   N+M_in:N+M_in+M_eq ]\n+        xi         = xi+alpha*d\n+        lam        = lam + alpha*(lam_d - lam)\n+    return xi\n+\n \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n \n \n@@ -61,7 +99,7 @@ class EntropyKnapsackPGD():\n     def __init__(self, N, C, stepsize = 0.001, n_iter=1000):\n         super().__init__()\n \n-        alpha = stepsize\n+        self.alpha = stepsize\n         eps = 1.0\n         \n         G = torch.cat(  (-torch.eye(N), torch.eye(N)),0  )\n@@ -74,12 +112,11 @@ class EntropyKnapsackPGD():\n         self.obj_grad_batch = torch.func.vmap(lambda x,c: obj_grad_fn(x,c,eps), in_dims=(0,None))\n         \n         self.projection = get_projection_layer(A,b,G,h)\n-        self.projection_batch = lambda x: torch.stack([self.projection(x_i) for _, x_i in enumerate(x)])\n         \n-        self.fwd_solver = lambda c: fwd_solver(c, N, C, eps)[0]\n+        self.fwd_solver = lambda c: fwd_solver(c, N, C, eps)\n         self.fwd_solver_batch = lambda c: torch.stack([self.fwd_solver(c_i) for _, c_i in enumerate(c)])\n         \n-        self.update_step  = lambda c,x: PGD(x,c,eps,self.projection,self.obj_grad,alpha,n_iter=1).float()\n+        self.update_step  = lambda c,x: PGD(x,c,self.projection,self.obj_grad,self.alpha,n_iter=1).float()\n         self.lmlLayer = FoldOptLayer(self.fwd_solver_batch, self.update_step, n_iter=n_iter, backprop_rule='FPI')\n \n     def solve(self,c):\n@@ -89,56 +126,44 @@ class EntropyKnapsackPGD():\n \n \n \n-# class EntropyKnapsackSQP():\n-#     def __init__(self, N, C, stepsize=1.0, n_iter=1):\n-#         super().__init__()\n-\n-#         self.alpha = stepsize\n-#         self.max_iter = n_iter\n-#         eps = 1.0\n-#         entro_knapsack_pd = cvxpy_models.EntropyNormKnapsackCVX(N,C,eps)\n-#         #p,d = entro_knapsack_pd.solve(coeffs)\n-#         entro_knapsack_pd_cat = lambda coeffs: torch.cat( entro_knapsack_pd.solve(coeffs.detach()) )\n-#         entro_knapsack_pd_batch = unrolled_ops.BatchModeWrapper(entro_knapsack_pd_cat)\n-#         self.entro_knapsack_pd_blank = unrolled_ops.BlankFunctionWrapper(N,entro_knapsack_pd_batch.apply)\n-#         self.N = N\n-#         self.M_in = 2*N\n-#         self.M_eq = 1\n-#         self.grad_f_ent   = lambda x,c: -c + eps*(torch.log(x) + 1.0)\n-#         self.hess_f_ent   = lambda x,l,c:    eps*torch.diag( 1/x )\n-#         self.grad_f = self.grad_f_ent\n-#         self.hess_L = self.hess_f_ent\n-#         self.g, self.e, self.grad_g, self.grad_e = diff_opt_tools.get_constraint_fns_knapsack(C)\n-\n-#     def solve(self,c):\n-#         x_blank = self.entro_knapsack_pd_blank(c.double())\n-#         primal = x_blank[:,:self.N]\n-#         dual   = x_blank[:,self.N:self.N+self.M_in]  # There are 2N inequalities\n-#         return unrolled_ops.DiffSQP(c.double(), self.N, self.M_in, self.M_eq, self.g, self.e, self.grad_g, self.grad_e, self.grad_f, self.hess_L, self.alpha, self.max_iter, primal, dual ).float()\n-\n-\n-\n-\n-\n-# class EntropyKnapsackFPGD():\n-#     def __init__(self, N, C, stepsize = 0.001, n_iter=1000):\n-#         super().__init__()\n+class EntropyKnapsackSQP():\n+    def __init__(self, N, C, stepsize=1.0, n_iter=1):\n+        super().__init__()\n \n-#         alpha = stepsize\n-#         self.n_iter = n_iter\n-#         eps = 1.0\n-#         entro_knapsack_pd = cvxpy_models.EntropyNormKnapsackCVX(N,C,eps)\n-#         entro_knapsack_pd_primal = lambda coeffs: entro_knapsack_pd.solve(coeffs.detach())[0]\n-#         entro_knapsack_pd_batch = unrolled_ops.BatchModeWrapper(entro_knapsack_pd_primal)\n-#         self.entro_knapsack_blank = unrolled_ops.BlankFunctionWrapper(N,entro_knapsack_pd_batch.apply)\n-#         self.fixedPtModule = fixedpt_ops.FixedPtDiff2()\n-#         grad_f_ent   = lambda x,c: -c + eps*(torch.log(x) + 1.0 )\n-#         A,b,G,h = diff_opt_tools.get_constraint_matrices_unwt_knapsack(N,C)\n-#         self.diff_step_op  = lambda clamb,xlamb: unrolled_ops.DiffPGD(clamb, N, grad_f_ent, G, h, A, b, alpha, 1, xlamb).float()\n+        self.alpha = stepsize\n+        eps = 1.0\n+        \n+        self.N = N\n+        self.M_in = 2*N\n+        self.M_eq = 1\n+        \n+        grad_f   = lambda x,c: -c + eps*(torch.log(x) + 1.0)\n+        hess_f   = lambda x,l,c:    eps*torch.diag( 1/x )\n+        grad_g   = lambda x: torch.cat( (-torch.eye(len(x)), torch.eye(len(x))),0 )\n+        grad_e   = lambda x: torch.ones(len(x)).unsqueeze(0)\n+        g        = lambda x: torch.cat( (-x,x-1) )\n+        e        = lambda x: (torch.sum(x) - C).unsqueeze(0)\n+        \n \n+        self.obj_grad = lambda x,c: obj_grad_fn(x,c,eps)\n+        self.obj_grad_batch = torch.func.vmap(lambda x,c: obj_grad_fn(x,c,eps), in_dims=(0,None))\n+        \n+        self.projection = get_sqp_layer(g,e,grad_g,grad_e,grad_f,hess_f)\n+        \n+        self.fwd_solver = lambda c,dual: fwd_solver(c,N,C,eps,primaldual=dual)\n+        \n+        def fwd_solver_batch(c, dual=False):\n+            if dual:\n+                primaldual = [self.fwd_solver(c_i,dual) for _, c_i in enumerate(c)]\n+                primal, dual = zip(*primaldual)\n+                return torch.cat([torch.stack(primal), torch.stack(dual)],dim=1)\n+            return torch.stack([self.fwd_solver(c_i,dual) for _, c_i in enumerate(c)])\n+        \n+        \n+        self.update_step  = lambda c,x: SQP(x,fwd_solver_batch,c,self.projection,self.N,self.M_in,self.M_eq,self.alpha,n_iter=1).float()\n+        self.lmlLayer = FoldOptLayer(fwd_solver_batch, self.update_step, n_iter=n_iter, backprop_rule='FPI')\n \n-#     def solve(self,c):\n-#         x_blank = self.entro_knapsack_blank(c.double())\n-#         jacobian_x, x_star_step = fixedpt_ops.iterate_fwd(c, x_blank, self.diff_step_op)\n-#         x_pgd  = self.fixedPtModule.apply(c, x_blank, x_star_step, jacobian_x, self.n_iter)\n-#         return x_pgd\n+    def solve(self,c):\n+        return self.lmlLayer( c )\n+        \n+        \ndiff --git a/multilabel_example/__pycache__/__init__.cpython-38.pyc b/multilabel_example/__pycache__/__init__.cpython-38.pyc\nindex 2459543..3ff02eb 100644\nBinary files a/multilabel_example/__pycache__/__init__.cpython-38.pyc and b/multilabel_example/__pycache__/__init__.cpython-38.pyc differ\ndiff --git a/multilabel_example/__pycache__/lml.cpython-38.pyc b/multilabel_example/__pycache__/lml.cpython-38.pyc\nindex 848278f..73e6abe 100644\nBinary files a/multilabel_example/__pycache__/lml.cpython-38.pyc and b/multilabel_example/__pycache__/lml.cpython-38.pyc differ\ndiff --git a/multilabel_example/__pycache__/multilabel_models.cpython-38.pyc b/multilabel_example/__pycache__/multilabel_models.cpython-38.pyc\nindex b8cb479..149ba20 100644\nBinary files a/multilabel_example/__pycache__/multilabel_models.cpython-38.pyc and b/multilabel_example/__pycache__/multilabel_models.cpython-38.pyc differ\ndiff --git a/multilabel_example/multilabel_models.py b/multilabel_example/multilabel_models.py\nindex b171457..50323ef 100644\n--- a/multilabel_example/multilabel_models.py\n+++ b/multilabel_example/multilabel_models.py\n@@ -10,6 +10,7 @@ from cvxpylayers.torch.cvxpylayer import CvxpyLayer\n import cvxpy as cp\n \n from multilabel_example.qpth_plus.qpth.qp import QPFunction\n+from multilabel_example.qpth_plus.qpth.qp import QPFunction_Plus\n \n \n \n@@ -17,10 +18,7 @@ from multilabel_example.qpth_plus.qpth.qp import QPFunction\n def obj_grad_fn(x,c,eps):\n     return -c + eps*(torch.log(x) + 1.0 )\n \n-def fwd_solver(c, N, C, eps):\n-    # c = c.clone().cpu().numpy()\n-    # print(type(c), c.shape)\n-    \n+def fwd_solver(c, N, C, eps, primaldual=False):\n     x = cp.Variable(N)\n     constraints = [  0<=x, x<=1, cp.sum(x) == C  ]\n     problem = cp.Problem(cp.Maximize(c @ x + eps*cp.sum(cp.entr(x))), constraints)\n@@ -28,7 +26,9 @@ def fwd_solver(c, N, C, eps):\n     solution = problem.solve(solver=cp.ECOS)\n     primal   = x.value\n     dual = np.concatenate( tuple([np.array([constr.dual_value]).flatten() for constr in constraints]) )\n-    return torch.Tensor(primal), torch.Tensor(dual)\n+    \n+    if primaldual: return torch.Tensor(primal), torch.Tensor(dual)\n+    return torch.Tensor(primal)\n \n # def get_projection_layer(A,b,G,h):\n #     N = A.shape[1] # number of variables\n@@ -41,18 +41,56 @@ def fwd_solver(c, N, C, eps):\n #     cvxlayer_post = lambda z: cvxlayer(z)[0]\n #     return cvxlayer_post\n \n+\n def get_projection_layer(A,b,G,h):\n     N = A.shape[1] # number of variables  \n     Q = torch.eye(N)\n     return lambda x: QPFunction(verbose=-1,eps=1e-10,maxIter=1000)(2*Q.double(),-2*x.double(),G.double(),h.double(),A.double(),b.double())   \n \n+def get_sqp_layer(g, e, grad_g, grad_e, grad_f, hess_L):\n+    \n+    def sqp_layer(x,c,lam):\n+        G = torch.stack( [grad_g(x_i).double() for x_i in x] )\n+        h = torch.stack( [-g(x_i).double() for x_i in x] )\n+        if e != None:\n+            A = torch.stack( [grad_e(x_i).double() for x_i in x] )\n+            b = torch.stack( [-e(x_i).double() for x_i in x] )\n+        else:\n+            A = torch.Tensor([])\n+            b = torch.Tensor([])\n+\n+        Q = torch.stack( [hess_L(x[i],lam[i],c[i] ).double()    for i in range(len(x))] )\n+        p = torch.stack( [grad_f(x_i,c[i]).double() for i,x_i in enumerate(x)] )\n+\n+        #The Hessian is H=2Q, the SQP minimizes (1/2)xT H x, Amos puts in (1/2) automatically\n+        primaldual = QPFunction_Plus(verbose=-1,eps=1e-14,maxIter=1000)(Q,p,G,h,A,b)\n+        return primaldual\n+    \n+    return sqp_layer\n+\n+\n # All data inputs are expected to be torch Tensors\n-def PGD(x0,c,eps,projection,obj_grad,alpha=0.01,n_iter=1):\n+def PGD(x0,c,projection,obj_grad,alpha=0.01,n_iter=1):\n     xi = x0\n     for i in range(n_iter):\n         grads = obj_grad(xi,c)\n         xi = projection(xi - alpha*grads)\n     return xi\n+\n+\n+def SQP(x0,fwd,c,projection,N,M_in,M_eq,alpha=0.01,n_iter=1):\n+    primaldual = fwd(c.clone().detach(), dual=True)\n+    xi = x0\n+    lam = primaldual[:,N:N+M_in]\n+    for i in range(n_iter):\n+        primaldual = projection(xi,c,lam)\n+        d          = primaldual[ :,:N ]\n+        lam_d      = primaldual[ :, N:N+M_in ]\n+        nu_d       = primaldual[ :,   N+M_in:N+M_in+M_eq ]\n+        xi         = xi+alpha*d\n+        lam        = lam + alpha*(lam_d - lam)\n+    return xi\n+\n \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n \n \n@@ -61,7 +99,7 @@ class EntropyKnapsackPGD():\n     def __init__(self, N, C, stepsize = 0.001, n_iter=1000):\n         super().__init__()\n \n-        alpha = stepsize\n+        self.alpha = stepsize\n         eps = 1.0\n         \n         G = torch.cat(  (-torch.eye(N), torch.eye(N)),0  )\n@@ -74,12 +112,11 @@ class EntropyKnapsackPGD():\n         self.obj_grad_batch = torch.func.vmap(lambda x,c: obj_grad_fn(x,c,eps), in_dims=(0,None))\n         \n         self.projection = get_projection_layer(A,b,G,h)\n-        self.projection_batch = lambda x: torch.stack([self.projection(x_i) for _, x_i in enumerate(x)])\n         \n-        self.fwd_solver = lambda c: fwd_solver(c, N, C, eps)[0]\n+        self.fwd_solver = lambda c: fwd_solver(c, N, C, eps)\n         self.fwd_solver_batch = lambda c: torch.stack([self.fwd_solver(c_i) for _, c_i in enumerate(c)])\n         \n-        self.update_step  = lambda c,x: PGD(x,c,eps,self.projection,self.obj_grad,alpha,n_iter=1).float()\n+        self.update_step  = lambda c,x: PGD(x,c,self.projection,self.obj_grad,self.alpha,n_iter=1).float()\n         self.lmlLayer = FoldOptLayer(self.fwd_solver_batch, self.update_step, n_iter=n_iter, backprop_rule='FPI')\n \n     def solve(self,c):\n@@ -89,56 +126,44 @@ class EntropyKnapsackPGD():\n \n \n \n-# class EntropyKnapsackSQP():\n-#     def __init__(self, N, C, stepsize=1.0, n_iter=1):\n-#         super().__init__()\n-\n-#         self.alpha = stepsize\n-#         self.max_iter = n_iter\n-#         eps = 1.0\n-#         entro_knapsack_pd = cvxpy_models.EntropyNormKnapsackCVX(N,C,eps)\n-#         #p,d = entro_knapsack_pd.solve(coeffs)\n-#         entro_knapsack_pd_cat = lambda coeffs: torch.cat( entro_knapsack_pd.solve(coeffs.detach()) )\n-#         entro_knapsack_pd_batch = unrolled_ops.BatchModeWrapper(entro_knapsack_pd_cat)\n-#         self.entro_knapsack_pd_blank = unrolled_ops.BlankFunctionWrapper(N,entro_knapsack_pd_batch.apply)\n-#         self.N = N\n-#         self.M_in = 2*N\n-#         self.M_eq = 1\n-#         self.grad_f_ent   = lambda x,c: -c + eps*(torch.log(x) + 1.0)\n-#         self.hess_f_ent   = lambda x,l,c:    eps*torch.diag( 1/x )\n-#         self.grad_f = self.grad_f_ent\n-#         self.hess_L = self.hess_f_ent\n-#         self.g, self.e, self.grad_g, self.grad_e = diff_opt_tools.get_constraint_fns_knapsack(C)\n-\n-#     def solve(self,c):\n-#         x_blank = self.entro_knapsack_pd_blank(c.double())\n-#         primal = x_blank[:,:self.N]\n-#         dual   = x_blank[:,self.N:self.N+self.M_in]  # There are 2N inequalities\n-#         return unrolled_ops.DiffSQP(c.double(), self.N, self.M_in, self.M_eq, self.g, self.e, self.grad_g, self.grad_e, self.grad_f, self.hess_L, self.alpha, self.max_iter, primal, dual ).float()\n-\n-\n-\n-\n-\n-# class EntropyKnapsackFPGD():\n-#     def __init__(self, N, C, stepsize = 0.001, n_iter=1000):\n-#         super().__init__()\n+class EntropyKnapsackSQP():\n+    def __init__(self, N, C, stepsize=1.0, n_iter=1):\n+        super().__init__()\n \n-#         alpha = stepsize\n-#         self.n_iter = n_iter\n-#         eps = 1.0\n-#         entro_knapsack_pd = cvxpy_models.EntropyNormKnapsackCVX(N,C,eps)\n-#         entro_knapsack_pd_primal = lambda coeffs: entro_knapsack_pd.solve(coeffs.detach())[0]\n-#         entro_knapsack_pd_batch = unrolled_ops.BatchModeWrapper(entro_knapsack_pd_primal)\n-#         self.entro_knapsack_blank = unrolled_ops.BlankFunctionWrapper(N,entro_knapsack_pd_batch.apply)\n-#         self.fixedPtModule = fixedpt_ops.FixedPtDiff2()\n-#         grad_f_ent   = lambda x,c: -c + eps*(torch.log(x) + 1.0 )\n-#         A,b,G,h = diff_opt_tools.get_constraint_matrices_unwt_knapsack(N,C)\n-#         self.diff_step_op  = lambda clamb,xlamb: unrolled_ops.DiffPGD(clamb, N, grad_f_ent, G, h, A, b, alpha, 1, xlamb).float()\n+        self.alpha = stepsize\n+        eps = 1.0\n+        \n+        self.N = N\n+        self.M_in = 2*N\n+        self.M_eq = 1\n+        \n+        grad_f   = lambda x,c: -c + eps*(torch.log(x) + 1.0)\n+        hess_f   = lambda x,l,c:    eps*torch.diag( 1/x )\n+        grad_g   = lambda x: torch.cat( (-torch.eye(len(x)), torch.eye(len(x))),0 )\n+        grad_e   = lambda x: torch.ones(len(x)).unsqueeze(0)\n+        g        = lambda x: torch.cat( (-x,x-1) )\n+        e        = lambda x: (torch.sum(x) - C).unsqueeze(0)\n+        \n \n+        self.obj_grad = lambda x,c: obj_grad_fn(x,c,eps)\n+        self.obj_grad_batch = torch.func.vmap(lambda x,c: obj_grad_fn(x,c,eps), in_dims=(0,None))\n+        \n+        self.projection = get_sqp_layer(g,e,grad_g,grad_e,grad_f,hess_f)\n+        \n+        self.fwd_solver = lambda c,dual: fwd_solver(c,N,C,eps,primaldual=dual)\n+        \n+        def fwd_solver_batch(c, dual=False):\n+            if dual:\n+                primaldual = [self.fwd_solver(c_i,dual) for _, c_i in enumerate(c)]\n+                primal, dual = zip(*primaldual)\n+                return torch.cat([torch.stack(primal), torch.stack(dual)],dim=1)\n+            return torch.stack([self.fwd_solver(c_i,dual) for _, c_i in enumerate(c)])\n+        \n+        \n+        self.update_step  = lambda c,x: SQP(x,fwd_solver_batch,c,self.projection,self.N,self.M_in,self.M_eq,self.alpha,n_iter=1).float()\n+        self.lmlLayer = FoldOptLayer(fwd_solver_batch, self.update_step, n_iter=n_iter, backprop_rule='FPI')\n \n-#     def solve(self,c):\n-#         x_blank = self.entro_knapsack_blank(c.double())\n-#         jacobian_x, x_star_step = fixedpt_ops.iterate_fwd(c, x_blank, self.diff_step_op)\n-#         x_pgd  = self.fixedPtModule.apply(c, x_blank, x_star_step, jacobian_x, self.n_iter)\n-#         return x_pgd\n+    def solve(self,c):\n+        return self.lmlLayer( c )\n+        \n+        \ndiff --git a/multilabel_example/qpth_plus/qpth/__pycache__/__init__.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/__pycache__/__init__.cpython-38.pyc\nindex 6df7284..c15c5a4 100644\nBinary files a/multilabel_example/qpth_plus/qpth/__pycache__/__init__.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/__pycache__/__init__.cpython-38.pyc differ\ndiff --git a/multilabel_example/qpth_plus/qpth/__pycache__/qp.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/__pycache__/qp.cpython-38.pyc\nindex be87027..b68d07b 100644\nBinary files a/multilabel_example/qpth_plus/qpth/__pycache__/qp.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/__pycache__/qp.cpython-38.pyc differ\ndiff --git a/multilabel_example/qpth_plus/qpth/__pycache__/util.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/__pycache__/util.cpython-38.pyc\nindex c591ac0..5a18d9f 100644\nBinary files a/multilabel_example/qpth_plus/qpth/__pycache__/util.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/__pycache__/util.cpython-38.pyc differ\ndiff --git a/multilabel_example/qpth_plus/qpth/solvers/__pycache__/__init__.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/solvers/__pycache__/__init__.cpython-38.pyc\nindex 3e174e1..29c55aa 100644\nBinary files a/multilabel_example/qpth_plus/qpth/solvers/__pycache__/__init__.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/solvers/__pycache__/__init__.cpython-38.pyc differ\ndiff --git a/multilabel_example/qpth_plus/qpth/solvers/__pycache__/cvxpy.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/solvers/__pycache__/cvxpy.cpython-38.pyc\nindex 2e67d1b..167cc86 100644\nBinary files a/multilabel_example/qpth_plus/qpth/solvers/__pycache__/cvxpy.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/solvers/__pycache__/cvxpy.cpython-38.pyc differ\ndiff --git a/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/__init__.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/__init__.cpython-38.pyc\nindex 187b2ee..db25968 100644\nBinary files a/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/__init__.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/__init__.cpython-38.pyc differ\ndiff --git a/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/batch.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/batch.cpython-38.pyc\nindex 2a0cf1f..c5ead31 100644\nBinary files a/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/batch.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/batch.cpython-38.pyc differ\ndiff --git a/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/spbatch.cpython-38.pyc b/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/spbatch.cpython-38.pyc\nindex 69f19d0..69a14f1 100644\nBinary files a/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/spbatch.cpython-38.pyc and b/multilabel_example/qpth_plus/qpth/solvers/pdipm/__pycache__/spbatch.cpython-38.pyc differ\ndiff --git a/multilabel_example/smooth-topk/src/losses/lml_loss.py b/multilabel_example/smooth-topk/src/losses/lml_loss.py\nindex 415bb06..501b735 100644\n--- a/multilabel_example/smooth-topk/src/losses/lml_loss.py\n+++ b/multilabel_example/smooth-topk/src/losses/lml_loss.py\n@@ -2,7 +2,7 @@ import torch\n from torch import nn\n \n \n-from multilabel_example.multilabel_models import EntropyKnapsackPGD #, EntropyKnapsackSQP, EntropyKnapsackFPGD\n+from multilabel_example.multilabel_models import EntropyKnapsackPGD, EntropyKnapsackSQP\n from cvxpylayers.torch.cvxpylayer import CvxpyLayer\n import cvxpy as cp\n \n@@ -82,20 +82,4 @@ class CVXLoss(nn.Module):\n         p = self.entro_knapsack_cvx(x/self.tau)\n         losses = -torch.log(p.gather(1, y.unsqueeze(1)) + 1e-8)\n         return losses.mean()\n-    \n-    \n-\n-class FPGDLoss(nn.Module):\n-    def __init__(self, n_classes, k=5, tau=1.0, alpha = 0.025, n_iter = 80):\n-        super(PGDLoss, self).__init__()\n-        self.n_classes = n_classes\n-        self.k = k\n-        self.tau = tau\n-        self.entknap_fixed = EntropyKnapsackPGD(n_classes, k, stepsize = alpha, n_iter=n_iter).solve\n-\n-    def forward(self, x, y):\n-        n_batch = x.shape[0]\n-        x = nn.functional.normalize(x)\n-        p = self.entknap_fixed(x/self.tau)\n-        losses = -torch.log(p.gather(1, y.unsqueeze(1)) + 1e-8)\n-        return losses.mean()\n\\ No newline at end of file\n+    \n\\ No newline at end of file\ndiff --git a/multilabel_example/smooth-topk/src/losses/main.py b/multilabel_example/smooth-topk/src/losses/main.py\nindex df060e8..9a113c0 100644\n--- a/multilabel_example/smooth-topk/src/losses/main.py\n+++ b/multilabel_example/smooth-topk/src/losses/main.py\n@@ -1,6 +1,6 @@\n import torch.nn as nn\n from losses.svm import SmoothSVM\n-from losses.lml_loss import LMLLoss, PGDLoss, CVXLoss, SQPLoss, FPGDLoss\n+from losses.lml_loss import LMLLoss, PGDLoss, CVXLoss, SQPLoss\n from losses.ml import MLLoss\n from losses.entr import EntrLoss\n \n@@ -18,9 +18,6 @@ def get_loss(xp, args):\n     elif args.loss == 'pgd': \n         print(\"Using PGD loss\")\n         loss = PGDLoss(n_classes=args.num_classes, k=args.topk, tau=args.tau, n_iter=args.num_steps)\n-    elif args.loss == 'fpgd': \n-        print(\"Using FPGD loss\")\n-        loss = FPGDLoss(n_classes=args.num_classes, k=args.topk, tau=args.tau, n_iter=args.num_steps)\n     elif args.loss == 'cvx':\n         print(\"Using cvx loss\")\n         loss = CVXLoss(n_classes=args.num_classes, k=args.topk, tau=args.tau)", "dataset": "cifar100", "data_root": null, "train_size": 2000, "val_size": 5000, "noise_labels": 1, "augment": 1, "multiple_crops": false, "use_dali": false, "model": "densenet_40_40", "load_model": null, "load_optimizer": null, "loss": "pgd", "topk": 5, "alpha": 1.0, "tau": 1, "mu": 0.0001, "num_steps": 60, "cuda": false, "device": 0, "parallel_gpu": false, "lr_0": 0.1, "batch_size": 64, "test_batch_size": 64, "epochs": 30, "lr_schedule": [151, 226], "out_name": "test/cifar100/losspgd_top5_epochs30_samples2000", "seed": 0, "server": "http://atlas.robots.ox.ac.uk", "port": 9001, "verbosity": 1, "log": true, "debug": false, "eval": false, "visdom": false, "num_classes": 100, "command_line": "multilabel_example/smooth-topk/src/main.py --loss pgd --train-size 2000 --model densenet_40_40 --epochs 30 --dataset cifar100 --topk 5 --out-name test", "cwd": "/sfs/qumulo/qhome/csk4sr/fold-opt.github.io", "hostname": "udc-ba26-34c0"}, "logged": {}, "name": "losspgd_top5_epochs30_samples2000", "name_and_dir": "test/cifar100/losspgd_top5_epochs30_samples2000", "date_and_time": "05-03-2024--16-44-33"}